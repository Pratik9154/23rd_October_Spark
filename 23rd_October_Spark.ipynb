{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63549428-e9be-4027-b505-44875e0fb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and\n",
    "   storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
    "   \n",
    "   - HDFS (Hadoop Distributed File System): A distributed file system designed to store large datasets reliably\n",
    "     and provide high-throughput access to data.\n",
    "   \n",
    "   - MapReduce: A programming model for processing large datasets in parallel across a Hadoop cluster. It divides\n",
    "     tasks into Map and Reduce phases.\n",
    "   \n",
    "   - YARN (Yet Another Resource Negotiator): Manages cluster resources and schedules applications, decoupling\n",
    "     resource management from data processing.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a\n",
    "   distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and\n",
    "   how they contribute to data reliability and fault tolerance.\n",
    "   \n",
    "   - NameNode: Manages metadata and the directory structure of HDFS. It keeps track of where data blocks are stored.\n",
    "   \n",
    "   - DataNode: Stores actual data in blocks. DataNodes periodically report back to the NameNode with information on\n",
    "     the blocks they store.\n",
    "   \n",
    "   - Blocks: Files in HDFS are split into blocks, typically 128 MB in size, and distributed across DataNodes.\n",
    "     This design allows HDFS to achieve fault tolerance and high availability by replicating blocks across different DataNodes.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to\n",
    "   illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing\n",
    "   large datasets.\n",
    "   \n",
    "   - Map Phase: The input data is split into smaller chunks, and a map function processes each chunk to generate key-value pairs.\n",
    "   \n",
    "   - Reduce Phase: The reduce function aggregates and processes the key-value pairs generated by the map function.\n",
    "   \n",
    "   - Example: Counting the number of occurrences of each word in a large text file.\n",
    "   \n",
    "   - Advantages: Scalability, fault tolerance, and simplicity in processing large datasets.\n",
    "   \n",
    "   - Limitations: Inefficient for iterative algorithms and slow due to disk I/O between Map and Reduce phases.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n",
    "   Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
    "   \n",
    "   - YARN separates resource management from the processing logic, allowing for more flexible and efficient use of cluster resources.\n",
    "   \n",
    "   - In Hadoop 1.x, MapReduce was responsible for both processing and resource management, leading to scalability issues.\n",
    "   \n",
    "   - YARN allows multiple data processing frameworks to run on Hadoop, not just MapReduce, making Hadoop a more versatile platform.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig,\n",
    "   and Spark. Describe the use cases and differences between these components. Choose one component and\n",
    "   explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
    "   \n",
    "   - HBase: A NoSQL database built on HDFS, designed for real-time read/write access to large datasets.\n",
    "   \n",
    "   - Hive: A data warehouse infrastructure that provides SQL-like querying on Hadoop data using a language called HiveQL.\n",
    "   \n",
    "   - Pig: A high-level platform for creating MapReduce programs using a scripting language called Pig Latin.\n",
    "   \n",
    "   - Spark: A fast, in-memory data processing engine that can run on HDFS and other storage systems, providing support for batch and streaming data.\n",
    "   \n",
    "   - Example of Integration: Hive can be used to query and analyze large datasets stored in HDFS using SQL-like syntax, making it accessible to users who are familiar with SQL.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome\n",
    "   some of the limitations of MapReduce for big data processing tasks?\n",
    "   \n",
    "   - Spark performs in-memory processing, which is much faster than the disk-based processing of MapReduce.\n",
    "   \n",
    "   - Spark is more efficient for iterative algorithms and interactive queries, as it can cache data in memory across iterations.\n",
    "   \n",
    "   - Spark's DAG (Directed Acyclic Graph) execution engine allows for more complex workflows than the simple MapReduce model.\n",
    "   \n",
    "   - Spark supports a wider range of workloads, including batch processing, streaming, machine learning, and graph processing.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word,\n",
    "   and returns the top 10 most frequent words. Explain the key components and steps involved in this\n",
    "   application.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"Word Count App\")\n",
    "\n",
    "# Read text file\n",
    "text_file = sc.textFile(\"example.txt\")\n",
    "\n",
    "# Word count process\n",
    "counts = (text_file.flatMap(lambda line: line.split(\" \"))\n",
    "                    .map(lambda word: (word, 1))\n",
    "                    .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "# Get top 10 most frequent words\n",
    "top_words = counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print(\"Top 10 words:\", top_words)\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "\"\"\"\n",
    "8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your\n",
    "   choice:\n",
    "   a. Filter the data to select only rows that meet specific criteria.\n",
    "   b. Map a transformation to modify a specific column in the dataset.\n",
    "   c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average).\n",
    "\"\"\"\n",
    "\n",
    "# Example RDD operations (Assuming 'rdd' is an RDD already loaded with data)\n",
    "\n",
    "# a. Filter the data to select only rows that meet specific criteria\n",
    "filtered_rdd = rdd.filter(lambda row: row['age'] > 30)\n",
    "\n",
    "# b. Map a transformation to modify a specific column in the dataset\n",
    "mapped_rdd = filtered_rdd.map(lambda row: (row['name'], row['age'] + 5))\n",
    "\n",
    "# c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum)\n",
    "total_age = mapped_rdd.map(lambda row: row[1]).reduce(lambda a, b: a + b)\n",
    "\n",
    "\"\"\"\n",
    "9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the\n",
    "   following operations:\n",
    "   a. Select specific columns from the DataFrame.\n",
    "   b. Filter rows based on certain conditions.\n",
    "   c. Group the data by a particular column and calculate aggregations (e.g., sum, average).\n",
    "   d. Join two DataFrames based on a common key.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# a. Select specific columns\n",
    "selected_df = df.select(\"name\", \"age\")\n",
    "\n",
    "# b. Filter rows based on certain conditions\n",
    "filtered_df = selected_df.filter(df[\"age\"] > 30)\n",
    "\n",
    "# c. Group the data by a particular column and calculate aggregations\n",
    "grouped_df = filtered_df.groupBy(\"name\").agg({\"age\": \"mean\"})\n",
    "\n",
    "# d. Join two DataFrames based on a common key\n",
    "df2 = spark.read.csv(\"data2.csv\", header=True, inferSchema=True)\n",
    "joined_df = df.join(df2, df[\"name\"] == df2[\"name\"])\n",
    "\n",
    "\"\"\"\n",
    "10. Set up a Spark Streaming application to process real-time data from a source (e.g., Apache Kafka or a\n",
    "    simulated data source). The application should:\n",
    "    a. Ingest data in micro-batches.\n",
    "    b. Apply a transformation to the streaming data (e.g., filtering, aggregation).\n",
    "    c. Output the processed data to a sink (e.g., write to a file, a database, or display it).\n",
    "\"\"\"\n",
    "\n",
    "# Example Spark Streaming application (Pseudocode)\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Initialize SparkContext and StreamingContext\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)  # 1-second batch interval\n",
    "\n",
    "# a. Ingest data in micro-batches from a source\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# b. Apply a transformation to the streaming data\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# c. Output the processed data to a sink (e.g., console output)\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\"\"\"\n",
    "11. Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in\n",
    "    the context of big data and real-time data processing?\n",
    "    \n",
    "    - Kafka is a distributed streaming platform designed to handle real-time data feeds.\n",
    "    \n",
    "    - It solves problems related to real-time data processing, high-throughput data streams, and the need for reliable,\n",
    "      scalable data pipelines.\n",
    "    \n",
    "    - Kafka provides a unified platform for handling real-time data feeds with features such as message retention,\n",
    "      partitioning, and fault tolerance.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "12. Describe the architecture of Kafka, including its key components such as Producers, Topics, Brokers,\n",
    "    Consumers, and ZooKeeper. How do these components work together in a Kafka cluster to achieve data\n",
    "    streaming?\n",
    "    \n",
    "    - Producers: Send data to Kafka topics.\n",
    "    \n",
    "    - Topics: Categories where data is stored, divided into partitions for scalability.\n",
    "    \n",
    "    - Brokers: Kafka servers that manage storage and retrieval of data, each responsible for a subset of partitions.\n",
    "    \n",
    "    - Consumers: Read data from topics, either individually or as part of a consumer group.\n",
    "    \n",
    "    - ZooKeeper: Manages the distributed environment of Kafka, coordinating the brokers and maintaining metadata.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "13. Create a step-by-step guide on how to produce data to a Kafka topic using a programming language of\n",
    "    your choice and then consume that data from the topic. Explain the role of Kafka producers and consumers\n",
    "    in this process.\n",
    "\"\"\"\n",
    "\n",
    "# Example using Python with Kafka (pseudocode)\n",
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Producing data to Kafka topic\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('my_topic', b'Hello, Kafka!')\n",
    "\n",
    "# Consuming data from Kafka topic\n",
    "consumer = KafkaConsumer('my_topic', bootstrap_servers='localhost:9092')\n",
    "for message in consumer:\n",
    "    print(message.value)\n",
    "\n",
    "\"\"\"\n",
    "14. Discuss the importance of data retention and data partitioning in Kafka. How can these features be\n",
    "    configured, and what are the implications for data storage and processing?\n",
    "    \n",
    "    - Data Retention: Determines how long Kafka retains messages in a topic. Configurable via the `log.retention.hours`\n",
    "      and related properties.\n",
    "    \n",
    "    - Data Partitioning: Distributes messages across multiple partitions within a topic. This allows Kafka to scale out\n",
    "      by distributing load across multiple brokers.\n",
    "    \n",
    "    - Implications: Properly configured retention and partitioning ensure Kafka can handle large volumes of data while\n",
    "      maintaining performance and fault tolerance.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "15. Give examples of real-world use cases where Apache Kafka is employed. Discuss why Kafka is the\n",
    "    preferred choice in those scenarios, and what benefits it brings to the table.\n",
    "    \n",
    "    - Real-Time Analytics: Kafka is used to stream data from various sources for real-time processing and analytics.\n",
    "    \n",
    "    - Log Aggregation: Kafka aggregates logs from multiple services and makes them available to systems like Hadoop or\n",
    "      Spark for processing.\n",
    "    \n",
    "    - Event Sourcing: Kafka is used to store and replay events in systems that follow an event-driven architecture.\n",
    "    \n",
    "    - Benefits: Kafka provides high-throughput, fault-tolerant, and scalable message streaming, making it ideal for\n",
    "      scenarios requiring reliable real-time data processing.\n",
    "\"\"\"\n",
    "\n",
    "# End of script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
